name: "Darkent2Caffe"
input: "data"
input_dim: 1
input_dim: 3
input_dim: 288
input_dim: 352

layer {
    bottom: "data"
    top: "layer1-conv"
    name: "layer1-conv"
    type: "Convolution"
    convolution_param {
        num_output: 8
        kernel_size: 3
        group: 1
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-act"
    type: "ReLU"
}
layer {
    bottom: "layer1-conv"
    top: "layer2-conv"
    name: "layer2-conv"
    type: "Convolution"
    convolution_param {
        num_output: 8
        kernel_size: 3
        group: 8
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer2-conv"
    top: "layer2-conv"
    name: "layer2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer2-conv"
    top: "layer2-conv"
    name: "layer2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer2-conv"
    top: "layer2-conv"
    name: "layer2-act"
    type: "ReLU"
}
layer {
    bottom: "layer2-conv"
    top: "layer3-conv"
    name: "layer3-conv"
    type: "Convolution"
    convolution_param {
        num_output: 4
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer4-conv"
    name: "layer4-conv"
    type: "Convolution"
    convolution_param {
        num_output: 18
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-act"
    type: "ReLU"
}
layer {
    bottom: "layer4-conv"
    top: "layer5-conv"
    name: "layer5-conv"
    type: "Convolution"
    convolution_param {
        num_output: 18
        kernel_size: 3
        group: 18
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer5-conv"
    top: "layer5-conv"
    name: "layer5-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer5-conv"
    top: "layer5-conv"
    name: "layer5-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer5-conv"
    top: "layer5-conv"
    name: "layer5-act"
    type: "ReLU"
}
layer {
    bottom: "layer5-conv"
    top: "layer6-conv"
    name: "layer6-conv"
    type: "Convolution"
    convolution_param {
        num_output: 6
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer7-conv"
    name: "layer7-conv"
    type: "Convolution"
    convolution_param {
        num_output: 36
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-act"
    type: "ReLU"
}
layer {
    bottom: "layer7-conv"
    top: "layer8-conv"
    name: "layer8-conv"
    type: "Convolution"
    convolution_param {
        num_output: 36
        kernel_size: 3
        group: 36
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer8-conv"
    top: "layer8-conv"
    name: "layer8-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer8-conv"
    top: "layer8-conv"
    name: "layer8-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer8-conv"
    top: "layer8-conv"
    name: "layer8-act"
    type: "ReLU"
}
layer {
    bottom: "layer8-conv"
    top: "layer9-conv"
    name: "layer9-conv"
    type: "Convolution"
    convolution_param {
        num_output: 6
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer9-conv"
    top: "layer9-conv"
    name: "layer9-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer9-conv"
    top: "layer9-conv"
    name: "layer9-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer6-conv"
    bottom: "layer9-conv"
    top: "layer10-shortcut"
    name: "layer10-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer10-shortcut"
    top: "layer11-conv"
    name: "layer11-conv"
    type: "Convolution"
    convolution_param {
        num_output: 18
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-act"
    type: "ReLU"
}
layer {
    bottom: "layer4-conv"
    top: "layer12-route"
    name: "layer12-route"
    type: "Concat"
}
layer {
    bottom: "layer12-route"
    top: "layer13-maxpool"
    name: "layer13-maxpool"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
        pad: 0
    }
}
layer {
    bottom: "layer13-maxpool"
    bottom: "layer11-conv"
    top: "layer14-route"
    name: "layer14-route"
    type: "Concat"
}
layer {
    bottom: "layer14-route"
    top: "layer15-conv"
    name: "layer15-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-act"
    type: "ReLU"
}
layer {
    bottom: "layer15-conv"
    top: "layer16-conv"
    name: "layer16-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 3
        group: 24
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer16-conv"
    top: "layer16-conv"
    name: "layer16-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer16-conv"
    top: "layer16-conv"
    name: "layer16-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer16-conv"
    top: "layer16-conv"
    name: "layer16-act"
    type: "ReLU"
}
layer {
    bottom: "layer16-conv"
    top: "layer17-conv"
    name: "layer17-conv"
    type: "Convolution"
    convolution_param {
        num_output: 8
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer17-conv"
    top: "layer17-conv"
    name: "layer17-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer17-conv"
    top: "layer17-conv"
    name: "layer17-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer17-conv"
    top: "layer18-conv"
    name: "layer18-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer18-conv"
    top: "layer18-conv"
    name: "layer18-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer18-conv"
    top: "layer18-conv"
    name: "layer18-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer18-conv"
    top: "layer18-conv"
    name: "layer18-act"
    type: "ReLU"
}
layer {
    bottom: "layer18-conv"
    top: "layer19-conv"
    name: "layer19-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 3
        group: 48
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer19-conv"
    top: "layer19-conv"
    name: "layer19-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer19-conv"
    top: "layer19-conv"
    name: "layer19-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer19-conv"
    top: "layer19-conv"
    name: "layer19-act"
    type: "ReLU"
}
layer {
    bottom: "layer19-conv"
    top: "layer20-conv"
    name: "layer20-conv"
    type: "Convolution"
    convolution_param {
        num_output: 8
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer20-conv"
    name: "layer20-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer20-conv"
    name: "layer20-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer17-conv"
    bottom: "layer20-conv"
    top: "layer21-shortcut"
    name: "layer21-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer21-shortcut"
    top: "layer22-conv"
    name: "layer22-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer22-conv"
    top: "layer22-conv"
    name: "layer22-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer22-conv"
    top: "layer22-conv"
    name: "layer22-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer22-conv"
    top: "layer22-conv"
    name: "layer22-act"
    type: "ReLU"
}
layer {
    bottom: "layer22-conv"
    top: "layer23-conv"
    name: "layer23-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 3
        group: 48
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-act"
    type: "ReLU"
}
layer {
    bottom: "layer23-conv"
    top: "layer24-conv"
    name: "layer24-conv"
    type: "Convolution"
    convolution_param {
        num_output: 8
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer24-conv"
    name: "layer24-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer24-conv"
    name: "layer24-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer21-shortcut"
    bottom: "layer24-conv"
    top: "layer25-shortcut"
    name: "layer25-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer25-shortcut"
    top: "layer26-conv"
    name: "layer26-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer26-conv"
    top: "layer26-conv"
    name: "layer26-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer26-conv"
    top: "layer26-conv"
    name: "layer26-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer26-conv"
    top: "layer26-conv"
    name: "layer26-act"
    type: "ReLU"
}
layer {
    bottom: "layer15-conv"
    top: "layer27-route"
    name: "layer27-route"
    type: "Concat"
}
layer {
    bottom: "layer27-route"
    top: "layer28-maxpool"
    name: "layer28-maxpool"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
        pad: 0
    }
}
layer {
    bottom: "layer28-maxpool"
    bottom: "layer26-conv"
    top: "layer29-route"
    name: "layer29-route"
    type: "Concat"
}
layer {
    bottom: "layer29-route"
    top: "layer30-conv"
    name: "layer30-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-act"
    type: "ReLU"
}
layer {
    bottom: "layer30-conv"
    top: "layer31-conv"
    name: "layer31-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 3
        group: 48
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer31-conv"
    top: "layer31-conv"
    name: "layer31-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer31-conv"
    top: "layer31-conv"
    name: "layer31-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer31-conv"
    top: "layer31-conv"
    name: "layer31-act"
    type: "ReLU"
}
layer {
    bottom: "layer31-conv"
    top: "layer32-conv"
    name: "layer32-conv"
    type: "Convolution"
    convolution_param {
        num_output: 16
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer33-conv"
    name: "layer33-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-act"
    type: "ReLU"
}
layer {
    bottom: "layer33-conv"
    top: "layer34-conv"
    name: "layer34-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 3
        group: 96
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer34-conv"
    top: "layer34-conv"
    name: "layer34-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer34-conv"
    top: "layer34-conv"
    name: "layer34-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer34-conv"
    top: "layer34-conv"
    name: "layer34-act"
    type: "ReLU"
}
layer {
    bottom: "layer34-conv"
    top: "layer35-conv"
    name: "layer35-conv"
    type: "Convolution"
    convolution_param {
        num_output: 16
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer35-conv"
    top: "layer35-conv"
    name: "layer35-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer35-conv"
    top: "layer35-conv"
    name: "layer35-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer32-conv"
    bottom: "layer35-conv"
    top: "layer36-shortcut"
    name: "layer36-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer36-shortcut"
    top: "layer37-conv"
    name: "layer37-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer37-conv"
    top: "layer37-conv"
    name: "layer37-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer37-conv"
    top: "layer37-conv"
    name: "layer37-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer37-conv"
    top: "layer37-conv"
    name: "layer37-act"
    type: "ReLU"
}
layer {
    bottom: "layer37-conv"
    top: "layer38-conv"
    name: "layer38-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 3
        group: 96
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer38-conv"
    top: "layer38-conv"
    name: "layer38-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer38-conv"
    top: "layer38-conv"
    name: "layer38-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer38-conv"
    top: "layer38-conv"
    name: "layer38-act"
    type: "ReLU"
}
layer {
    bottom: "layer38-conv"
    top: "layer39-conv"
    name: "layer39-conv"
    type: "Convolution"
    convolution_param {
        num_output: 16
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer39-conv"
    top: "layer39-conv"
    name: "layer39-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer39-conv"
    top: "layer39-conv"
    name: "layer39-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer36-shortcut"
    bottom: "layer39-conv"
    top: "layer40-shortcut"
    name: "layer40-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer40-shortcut"
    top: "layer41-conv"
    name: "layer41-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer41-conv"
    top: "layer41-conv"
    name: "layer41-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer41-conv"
    top: "layer41-conv"
    name: "layer41-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer41-conv"
    top: "layer41-conv"
    name: "layer41-act"
    type: "ReLU"
}
layer {
    bottom: "layer41-conv"
    top: "layer42-conv"
    name: "layer42-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 3
        group: 96
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer42-conv"
    top: "layer42-conv"
    name: "layer42-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer42-conv"
    top: "layer42-conv"
    name: "layer42-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer42-conv"
    top: "layer42-conv"
    name: "layer42-act"
    type: "ReLU"
}
layer {
    bottom: "layer42-conv"
    top: "layer43-conv"
    name: "layer43-conv"
    type: "Convolution"
    convolution_param {
        num_output: 16
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer43-conv"
    top: "layer43-conv"
    name: "layer43-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer43-conv"
    top: "layer43-conv"
    name: "layer43-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer40-shortcut"
    bottom: "layer43-conv"
    top: "layer44-shortcut"
    name: "layer44-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer44-shortcut"
    top: "layer45-conv"
    name: "layer45-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer45-conv"
    top: "layer45-conv"
    name: "layer45-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer45-conv"
    top: "layer45-conv"
    name: "layer45-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer45-conv"
    top: "layer45-conv"
    name: "layer45-act"
    type: "ReLU"
}
layer {
    bottom: "layer30-conv"
    top: "layer46-route"
    name: "layer46-route"
    type: "Concat"
}
layer {
    bottom: "layer46-route"
    top: "layer47-maxpool"
    name: "layer47-maxpool"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
        pad: 0
    }
}
layer {
    bottom: "layer47-maxpool"
    bottom: "layer45-conv"
    top: "layer48-route"
    name: "layer48-route"
    type: "Concat"
}
layer {
    bottom: "layer48-route"
    top: "layer49-conv"
    name: "layer49-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer49-conv"
    top: "layer49-conv"
    name: "layer49-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer49-conv"
    top: "layer49-conv"
    name: "layer49-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer49-conv"
    top: "layer49-conv"
    name: "layer49-act"
    type: "ReLU"
}
layer {
    bottom: "layer49-conv"
    top: "layer50-conv"
    name: "layer50-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 3
        group: 96
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer50-conv"
    top: "layer50-conv"
    name: "layer50-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer50-conv"
    top: "layer50-conv"
    name: "layer50-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer50-conv"
    top: "layer50-conv"
    name: "layer50-act"
    type: "ReLU"
}
layer {
    bottom: "layer50-conv"
    top: "layer51-conv"
    name: "layer51-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer51-conv"
    top: "layer51-conv"
    name: "layer51-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer51-conv"
    top: "layer51-conv"
    name: "layer51-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer51-conv"
    top: "layer52-conv"
    name: "layer52-conv"
    type: "Convolution"
    convolution_param {
        num_output: 144
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer52-conv"
    top: "layer52-conv"
    name: "layer52-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer52-conv"
    top: "layer52-conv"
    name: "layer52-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer52-conv"
    top: "layer52-conv"
    name: "layer52-act"
    type: "ReLU"
}
layer {
    bottom: "layer52-conv"
    top: "layer53-conv"
    name: "layer53-conv"
    type: "Convolution"
    convolution_param {
        num_output: 144
        kernel_size: 3
        group: 144
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer53-conv"
    top: "layer53-conv"
    name: "layer53-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer53-conv"
    top: "layer53-conv"
    name: "layer53-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer53-conv"
    top: "layer53-conv"
    name: "layer53-act"
    type: "ReLU"
}
layer {
    bottom: "layer53-conv"
    top: "layer54-conv"
    name: "layer54-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer54-conv"
    top: "layer54-conv"
    name: "layer54-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer54-conv"
    top: "layer54-conv"
    name: "layer54-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer51-conv"
    bottom: "layer54-conv"
    top: "layer55-shortcut"
    name: "layer55-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer55-shortcut"
    top: "layer56-conv"
    name: "layer56-conv"
    type: "Convolution"
    convolution_param {
        num_output: 144
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer56-conv"
    top: "layer56-conv"
    name: "layer56-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer56-conv"
    top: "layer56-conv"
    name: "layer56-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer56-conv"
    top: "layer56-conv"
    name: "layer56-act"
    type: "ReLU"
}
layer {
    bottom: "layer56-conv"
    top: "layer57-conv"
    name: "layer57-conv"
    type: "Convolution"
    convolution_param {
        num_output: 144
        kernel_size: 3
        group: 144
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer57-conv"
    top: "layer57-conv"
    name: "layer57-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer57-conv"
    top: "layer57-conv"
    name: "layer57-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer57-conv"
    top: "layer57-conv"
    name: "layer57-act"
    type: "ReLU"
}
layer {
    bottom: "layer57-conv"
    top: "layer58-conv"
    name: "layer58-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer58-conv"
    top: "layer58-conv"
    name: "layer58-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer58-conv"
    top: "layer58-conv"
    name: "layer58-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer55-shortcut"
    bottom: "layer58-conv"
    top: "layer59-shortcut"
    name: "layer59-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer59-shortcut"
    top: "layer60-conv"
    name: "layer60-conv"
    type: "Convolution"
    convolution_param {
        num_output: 96
        kernel_size: 1
        group: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer60-conv"
    top: "layer60-conv"
    name: "layer60-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer60-conv"
    top: "layer60-conv"
    name: "layer60-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer60-conv"
    top: "layer60-conv"
    name: "layer60-act"
    type: "ReLU"
}
layer {
    bottom: "layer49-conv"
    top: "layer61-route"
    name: "layer61-route"
    type: "Concat"
}
layer {
    bottom: "layer61-route"
    top: "layer62-maxpool"
    name: "layer62-maxpool"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
        pad: 0
    }
}
layer {
    bottom: "layer62-maxpool"
    bottom: "layer60-conv"
    top: "layer63-route"
    name: "layer63-route"
    type: "Concat"
}
layer {
    bottom: "layer63-route"
    top: "layer64-conv"
    name: "layer64-conv"
    type: "Convolution"
    convolution_param {
        num_output: 72
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer64-conv"
    name: "layer64-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer64-conv"
    name: "layer64-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer64-conv"
    name: "layer64-act"
    type: "ReLU"
}
layer {
    bottom: "layer64-conv"
    top: "layer65-avgpool"
    name: "layer65-avgpool"
    type: "Pooling"
    pooling_param {
        pool: AVE
        global_pooling: true
    }
}
layer {
    bottom: "layer65-avgpool"
    top: "layer66-conv"
    name: "layer66-conv"
    type: "Convolution"
    convolution_param {
        num_output: 36
        kernel_size: 1
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer66-conv"
    top: "layer66-conv"
    name: "layer66-act"
    type: "ReLU"
}
layer {
    bottom: "layer66-conv"
    top: "layer67-conv"
    name: "layer67-conv"
    type: "Convolution"
    convolution_param {
        num_output: 72
        kernel_size: 1
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer67-conv"
    top: "layer67-conv"
    name: "layer67-act"
    type: "Sigmoid"
}
layer {
  name: "layer67-conv_flatten"
  type: "Flatten"
  bottom: "layer67-conv"
  top: "layer67-conv_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
    bottom: "layer64-conv"
    bottom: "layer67-conv_flatten"
    top: "layer68-scale_channels"
    name: "layer68-scale_channels"
    type: "Scale"
    scale_param {
        bias_term: false
        axis: 0
    }
}
layer {
    bottom: "layer68-scale_channels"
    top: "layer69-conv"
    name: "layer69-conv"
    type: "Convolution"
    convolution_param {
        num_output: 72
        kernel_size: 3
        group: 72
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer69-conv"
    top: "layer69-conv"
    name: "layer69-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer69-conv"
    top: "layer69-conv"
    name: "layer69-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer69-conv"
    top: "layer69-conv"
    name: "layer69-act"
    type: "ReLU"
}
layer {
    bottom: "layer69-conv"
    top: "layer70-conv"
    name: "layer70-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer70-conv"
    top: "layer70-conv"
    name: "layer70-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer70-conv"
    top: "layer70-conv"
    name: "layer70-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer70-conv"
    top: "layer70-conv"
    name: "layer70-act"
    type: "ReLU"
}
layer {
    bottom: "layer70-conv"
    top: "layer71-conv"
    name: "layer71-conv"
    type: "Convolution"
    convolution_param {
        num_output: 18
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer73-route"
    name: "layer73-route"
    type: "Concat"
}
layer {
    bottom: "layer73-route"
    top: "layer74-upsample"
    name: "layer74-upsample"
    type: "Upsample"
    upsample_param {
        scale: 2
    }
}
layer {
    bottom: "layer74-upsample"
    bottom: "layer49-conv"
    top: "layer75-route"
    name: "layer75-route"
    type: "Concat"
}
layer {
    bottom: "layer75-route"
    top: "layer76-conv"
    name: "layer76-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer76-conv"
    name: "layer76-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer76-conv"
    name: "layer76-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer76-conv"
    name: "layer76-act"
    type: "ReLU"
}
layer {
    bottom: "layer76-conv"
    top: "layer77-avgpool"
    name: "layer77-avgpool"
    type: "Pooling"
    pooling_param {
        pool: AVE
        global_pooling: true
    }
}
layer {
    bottom: "layer77-avgpool"
    top: "layer78-conv"
    name: "layer78-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer78-conv"
    top: "layer78-conv"
    name: "layer78-act"
    type: "ReLU"
}
layer {
    bottom: "layer78-conv"
    top: "layer79-conv"
    name: "layer79-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 1
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer79-conv"
    top: "layer79-conv"
    name: "layer79-act"
    type: "Sigmoid"
}
layer {
  name: "layer79-conv_flatten"
  type: "Flatten"
  bottom: "layer79-conv"
  top: "layer79-conv_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
    bottom: "layer76-conv"
    bottom: "layer79-conv_flatten"
    top: "layer80-scale_channels"
    name: "layer80-scale_channels"
    type: "Scale"
    scale_param {
        bias_term: false
        axis: 0
    }
}
layer {
    bottom: "layer80-scale_channels"
    top: "layer81-conv"
    name: "layer81-conv"
    type: "Convolution"
    convolution_param {
        num_output: 48
        kernel_size: 3
        group: 48
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer81-conv"
    top: "layer81-conv"
    name: "layer81-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer81-conv"
    top: "layer81-conv"
    name: "layer81-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer81-conv"
    top: "layer81-conv"
    name: "layer81-act"
    type: "ReLU"
}
layer {
    bottom: "layer81-conv"
    top: "layer82-conv"
    name: "layer82-conv"
    type: "Convolution"
    convolution_param {
        num_output: 36
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer82-conv"
    top: "layer82-conv"
    name: "layer82-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer82-conv"
    top: "layer82-conv"
    name: "layer82-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer82-conv"
    top: "layer82-conv"
    name: "layer82-act"
    type: "ReLU"
}
layer {
    bottom: "layer82-conv"
    top: "layer83-conv"
    name: "layer83-conv"
    type: "Convolution"
    convolution_param {
        num_output: 18
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer85-route"
    name: "layer85-route"
    type: "Concat"
}
layer {
    bottom: "layer85-route"
    top: "layer86-upsample"
    name: "layer86-upsample"
    type: "Upsample"
    upsample_param {
        scale: 2
    }
}
layer {
    bottom: "layer86-upsample"
    bottom: "layer30-conv"
    top: "layer87-route"
    name: "layer87-route"
    type: "Concat"
}
layer {
    bottom: "layer87-route"
    top: "layer88-conv"
    name: "layer88-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer88-conv"
    top: "layer88-conv"
    name: "layer88-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer88-conv"
    top: "layer88-conv"
    name: "layer88-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer88-conv"
    top: "layer88-conv"
    name: "layer88-act"
    type: "ReLU"
}
layer {
    bottom: "layer88-conv"
    top: "layer89-avgpool"
    name: "layer89-avgpool"
    type: "Pooling"
    pooling_param {
        pool: AVE
        global_pooling: true
    }
}
layer {
    bottom: "layer89-avgpool"
    top: "layer90-conv"
    name: "layer90-conv"
    type: "Convolution"
    convolution_param {
        num_output: 12
        kernel_size: 1
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer90-conv"
    top: "layer90-conv"
    name: "layer90-act"
    type: "ReLU"
}
layer {
    bottom: "layer90-conv"
    top: "layer91-conv"
    name: "layer91-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 1
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer91-conv"
    top: "layer91-conv"
    name: "layer91-act"
    type: "Sigmoid"
}
layer {
  name: "layer91-conv_flatten"
  type: "Flatten"
  bottom: "layer91-conv"
  top: "layer91-conv_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
    bottom: "layer88-conv"
    bottom: "layer91-conv_flatten"
    top: "layer92-scale_channels"
    name: "layer92-scale_channels"
    type: "Scale"
    scale_param {
        bias_term: false
        axis: 0
    }
}
layer {
    bottom: "layer92-scale_channels"
    top: "layer93-conv"
    name: "layer93-conv"
    type: "Convolution"
    convolution_param {
        num_output: 24
        kernel_size: 3
        group: 24
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer93-conv"
    top: "layer93-conv"
    name: "layer93-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer93-conv"
    top: "layer93-conv"
    name: "layer93-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer93-conv"
    top: "layer93-conv"
    name: "layer93-act"
    type: "ReLU"
}
layer {
    bottom: "layer93-conv"
    top: "layer94-conv"
    name: "layer94-conv"
    type: "Convolution"
    convolution_param {
        num_output: 30
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer94-conv"
    top: "layer94-conv"
    name: "layer94-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer94-conv"
    top: "layer94-conv"
    name: "layer94-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer94-conv"
    top: "layer94-conv"
    name: "layer94-act"
    type: "ReLU"
}
layer {
    bottom: "layer94-conv"
    top: "layer95-conv"
    name: "layer95-conv"
    type: "Convolution"
    convolution_param {
        num_output: 18
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
    }
}
